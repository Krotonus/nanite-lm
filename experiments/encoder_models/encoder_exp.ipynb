{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a58971a-f4c5-4498-93c8-62ac29acfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.attention.flex_attention import create_block_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45288fe6-1d36-4f04-9549-e6691a4a216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/data/horse/ws/lama722b-nanite-lm/nanite-lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0d53280-04a6-4c9c-8998-197b438d6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebase.transformer import Attention, FeedForward, RotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "798fe700-4aa8-49da-a5e0-a25ca9b65625",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PostModernBertArgs:\n",
    "    vocab_size: int = 256\n",
    "    dim: int = 768\n",
    "    n_layers: int = 22\n",
    "    head_dim: Optional[int] = None\n",
    "\n",
    "    # Embedding Related Params\n",
    "    pad_token_id: int = 255\n",
    "    norm_eps: float = 1e-5\n",
    "    norm_bias: float = False\n",
    "    embedding_dropout: float = 0.0\n",
    "\n",
    "    #Model Args\n",
    "    max_seqlen: int = 512\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[int] = None\n",
    "    rope_theta: float = 10_000.0\n",
    "    head_dim: Optional[int] = None\n",
    "    n_heads: int = 12\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa95509e-aa23-44ce-aad2-707b9ae19f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostModernBertEncoderBlock(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # ModernBERT uses nn.Identity as attn_norm for layer_idx ==0\n",
    "        self.attn_norm = nn.LayerNorm(args.dim, eps=args.norm_eps, bias=args.norm_bias)\n",
    "        self.head_dim = args.head_dim or args.dim // args.n_heads\n",
    "        self.n_heads = args.n_heads or args.dim // args.head_dim\n",
    "        self.n_kv_heads = args.n_kv_heads or self.n_heads\n",
    "        self.attn = Attention(args.dim,\n",
    "                              head_dim=self.head_dim,\n",
    "                              n_heads=self.n_heads,\n",
    "                              n_kv_heads=self.n_kv_heads,\n",
    "                              rope_theta=args.rope_theta\n",
    "                             )\n",
    "        self.mlp = FeedForward(args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of, ffn_dim_multiplier=args.ffn_dim_multiplier)\n",
    "        self.mlp_norm = nn.LayerNorm(args.dim, eps=args.norm_eps, bias=args.norm_bias)\n",
    "\n",
    "    def forward(self, x, freq_cis):\n",
    "        h = x + self.attn(x, freq_cis)\n",
    "        # Residual Connection\n",
    "        # MLP Output\n",
    "        out = h + self.mlp(self.mlp_norm(h))\n",
    "        return out\n",
    "\n",
    "class PostModernEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Currently similar to ModernBert without `torch.compile`\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.tok_embeddings = torch.nn.Embedding(args.vocab_size, args.dim, padding_idx=args.pad_token_id)\n",
    "        self.norm = nn.LayerNorm(args.dim, eps=args.norm_eps, bias=args.norm_bias)\n",
    "        self.drop = nn.Dropout(args.embedding_dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        h = self.drop(self.norm(self.tok_embeddings(input_ids)))\n",
    "        return h\n",
    "\n",
    "class PostModernBert(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.max_seqlen = args.max_seqlen\n",
    "        self.tok_embeddings = PostModernEmbeddings(args)\n",
    "        self.rope_embeddings = RotaryEmbedding(\n",
    "            theta=args.rope_theta,\n",
    "            head_dim=args.head_dim or args.dim // args.n_heads,\n",
    "            max_seqlen=args.max_seqlen,\n",
    "        )\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(args.n_layers):\n",
    "            self.layers.append(PostModernBertEncoderBlock(args))\n",
    "        self.final_norm = nn.LayerNorm(args.dim, eps=args.norm_eps, bias=args.norm_bias)\n",
    "\n",
    "    def forward(self, inp, attention_mask=None): \n",
    "        batch_size, seq_len = inp.shape[:2]\n",
    "        #TODO(krotonus): Move the below variable\n",
    "        tok_idx = None\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(\n",
    "                (batch_size, seq_len),\n",
    "            )\n",
    "        freq_cis = self.rope_embeddings(seqlen=self.max_seqlen, tok_idx=tok_idx)\n",
    "        h = self.tok_embeddings(inp)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h, freq_cis)\n",
    "        h = self.final_norm(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "da723e82-739f-4e14-bd4b-a4bc1e27d09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0859, -0.1358,  1.4519,  ...,  0.3180, -1.1336, -0.6670],\n",
       "         [-0.4470, -0.2797, -0.2225,  ...,  0.5280,  0.5631, -0.6856],\n",
       "         [-0.6883, -0.7299,  2.5745,  ...,  0.4008,  0.0605,  0.0774],\n",
       "         ...,\n",
       "         [-1.4310, -0.9367,  0.3493,  ...,  1.0679, -0.2719,  0.1380],\n",
       "         [-0.6686, -0.0198,  0.2524,  ..., -0.4789, -0.5860, -1.4794],\n",
       "         [ 0.3705, -0.3117,  1.2196,  ...,  1.1364, -0.8761, -0.8457]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = PostModernBertArgs()\n",
    "model = PostModernBert(args)\n",
    "inp = torch.randint(low=0, high=256, size=(1, 10))\n",
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d69f50f-7006-487f-982e-91a43659b882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostModernBert(\n",
       "  (tok_embeddings): PostModernEmbeddings(\n",
       "    (tok_embeddings): Embedding(256, 768, padding_idx=255)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (rope_embeddings): RotaryEmbedding()\n",
       "  (layers): ModuleList(\n",
       "    (0-21): 22 x PostModernBertEncoderBlock(\n",
       "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "        (w3): Linear(in_features=768, out_features=2048, bias=False)\n",
       "        (w2): Linear(in_features=2048, out_features=768, bias=False)\n",
       "      )\n",
       "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4115bbc7-ef4f-49ff-ad51-e6b5f41f5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "ref_config = AutoConfig.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d5670fc6-1870-4553-b441-a372cfb289d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBertModel(\n",
      "  (embeddings): ModernBertEmbeddings(\n",
      "    (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
      "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0): ModernBertEncoderLayer(\n",
      "      (attn_norm): Identity()\n",
      "      (attn): ModernBertAttention(\n",
      "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "        (rotary_emb): ModernBertRotaryEmbedding()\n",
      "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_drop): Identity()\n",
      "      )\n",
      "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): ModernBertMLP(\n",
      "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
      "        (act): GELUActivation()\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1-21): 21 x ModernBertEncoderLayer(\n",
      "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): ModernBertAttention(\n",
      "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "        (rotary_emb): ModernBertRotaryEmbedding()\n",
      "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_drop): Identity()\n",
      "      )\n",
      "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): ModernBertMLP(\n",
      "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
      "        (act): GELUActivation()\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6153, -0.8174, -0.5756,  ...,  1.8049,  1.5337,  1.3146],\n",
       "         [ 0.1290,  0.8749, -1.3029,  ...,  0.0916,  0.4125, -0.1262],\n",
       "         [ 0.4988, -0.9913, -0.5715,  ..., -1.4603, -1.0956, -0.5315],\n",
       "         ...,\n",
       "         [-2.2732,  0.6711, -0.0564,  ...,  1.1797, -0.6134, -0.3962],\n",
       "         [ 1.1997,  0.4882,  0.7147,  ..., -0.7807, -0.1498, -0.5456],\n",
       "         [-0.4344, -0.6453,  0.1459,  ...,  0.0474,  0.9059, -1.3447]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_model = AutoModel.from_config(ref_config)\n",
    "print(ref_model)\n",
    "ref_model(inp).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5cd194ff-aa47-427a-a0cd-70ba58d23f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.8827e-01, -7.6610e-01,  5.3896e-01,  ..., -4.8102e-01,\n",
       "          -8.7296e-01, -4.4662e-03],\n",
       "         [-3.5031e-01, -1.5203e+00, -1.4971e-03,  ..., -4.7378e-01,\n",
       "           6.1782e-01,  1.5122e+00],\n",
       "         [ 1.5161e-01, -1.7240e+00, -3.6489e-01,  ...,  1.1788e+00,\n",
       "           1.0126e+00,  2.6658e-01],\n",
       "         ...,\n",
       "         [-9.1646e-01,  2.1504e-01,  3.2687e-01,  ..., -6.3135e-01,\n",
       "           9.6177e-01,  8.9055e-01],\n",
       "         [-1.8178e-01, -5.7361e-01, -9.8695e-01,  ..., -9.3489e-01,\n",
       "           2.8982e-01,  1.2466e-02],\n",
       "         [-1.7038e+00, -1.3339e+00, -3.5118e-01,  ...,  6.0886e-01,\n",
       "          -6.5583e-01, -5.5107e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f6424c5-3dab-4fc8-8e06-9932ded87028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.attention.flex_attention as flex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9ec5731-ea54-4e20-ac5a-886b5e1f3861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockMask(\n",
       "    kv_num_blocks=torch.Size([2, 12, 4]),\n",
       "    kv_indices=torch.Size([2, 12, 4, 4]),\n",
       "    full_kv_num_blocks=torch.Size([2, 12, 4]),\n",
       "    full_kv_indices=torch.Size([2, 12, 4, 4]),\n",
       "    q_num_blocks=torch.Size([2, 12, 4]),\n",
       "    q_indices=torch.Size([2, 12, 4, 4]),\n",
       "    full_q_num_blocks=torch.Size([2, 12, 4]),\n",
       "    full_q_indices=torch.Size([2, 12, 4, 4]),\n",
       "    BLOCK_SIZE=(128, 128),\n",
       "    shape=(2, 12, 512, 512),\n",
       "    sparsity=0.00%,\n",
       "    mask_mod=mod_fn\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mod_fn(b, h, q_idx, kv_idx):\n",
    "    return q_idx != kv_idx\n",
    "\n",
    "B = 2\n",
    "H = 12 # Number of query heads\n",
    "q_len = 512\n",
    "kv_len = 512\n",
    "inp = torch.rand((B, H, q_len, kv_len))\n",
    "flex.create_block_mask(mod_fn, B, H, q_len, kv_len, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0517839-a600-46fd-ac60-b1f9e8fa772f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
