{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1c3671-acb2-4976-87c0-1694a94af1c1",
   "metadata": {},
   "source": [
    "# DoReMi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fc1d9-6c69-46bf-88bf-6736cc81b19d",
   "metadata": {},
   "source": [
    "## How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf7e57-08b6-4e76-bd5d-3ae8b8b00093",
   "metadata": {},
   "source": [
    "- Step 1: Train a small referece model using uniform sampling from each data domain (for a given batch size).\n",
    "- Step 2: Used the trained reference model from previous steps to train an identical mode, and use its performance to dynamically tune the domain weights.\n",
    "- Step 3: Save the domain weights in model checkpoint. Calculate the optimal domain weights by averaging the domain weights across all the training steps.\n",
    "- Step 4: Use the optimized domain weights from previous step to train a larger model. (10x-30x larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b28a8e-f4ba-4f26-8c44-8ca548d709eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/data/horse/ws/lama722b-nanite-lm/nanite-lm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1665c23e-cce6-4675-9cf0-2755249b1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b():\n",
    "    try:\n",
    "        from ipdb import set_trace\n",
    "    except:\n",
    "        from pdb import set_trace\n",
    "    set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b20c5b9-7e50-4ce8-ac50-8bf0e48478b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "from codebase.data import (\n",
    "    DataArgs,\n",
    "    build_dataloader_from_args,\n",
    "    init_dataloader_state_from_args\n",
    ")\n",
    "from codebase.tokenizer import (\n",
    "    build_tokenizer,\n",
    "    TokenizerArgs\n",
    ")\n",
    "from codebase.optim import (\n",
    "    build_optimizer,\n",
    "    OptimArgs\n",
    ")\n",
    "\n",
    "from codebase.transformer import (\n",
    "    BaseTransformer,\n",
    "    RMSNorm\n",
    ")\n",
    "\n",
    "from experiments.baseline_transformer.transformer import (\n",
    "    LMTransformerArgs,\n",
    "    LMTransformer,\n",
    "    create_causal_mask\n",
    ")\n",
    "\n",
    "NUM_TRAIN_STEPS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7555f2-1e24-49c2-acee-1045cfe0f1d4",
   "metadata": {},
   "source": [
    "### Per-Domain Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb240bbb-730a-4686-8584-2cf878481b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, target, **kwargs):\n",
    "    inp = F.log_softmax(pred.flatten(end_dim=-2).float(), -1)\n",
    "    return F.nll_loss(\n",
    "        inp,\n",
    "        target.flatten(end_dim=-1),\n",
    "        reduction = \"mean\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "def per_token_cross_entropy(pred, target, **kwargs):\n",
    "    inp = F.log_softmax(pred.flatten(end_dim=-2).float(), -1)\n",
    "    return F.nll_loss(\n",
    "        inp,\n",
    "        target.flatten(end_dim=-1),\n",
    "        reduction = \"none\",\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fed1e-b1ec-4aad-8dff-3f3d29355ea7",
   "metadata": {},
   "source": [
    "### DoReMi Context\n",
    "\n",
    "- This is used to maintain the doremi weights and thier history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80d8ce2-5b15-4f47-906b-2fbf124fd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, TypedDict\n",
    "\n",
    "class WeightHistory(TypedDict):\n",
    "    step: int\n",
    "    weight: torch.Tensor\n",
    "\n",
    "@dataclass\n",
    "class DoReMiContext:\n",
    "    # Note(krotonus): This is the current domain weights\n",
    "    domain_keys: List[str]\n",
    "    is_proxy: bool\n",
    "    step_size: float = 1\n",
    "    smoothing_param: float = 1e-3\n",
    "    domain_weight_history: WeightHistory = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def num_domain(self) -> int:\n",
    "        return len(self.domain_keys)\n",
    "\n",
    "    def get_domain_name(self, domain_idx: int) -> str:\n",
    "        return self.domain_keys[domain_idx]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.domain_weights = torch.ones(self.num_domains) / self.num_domains\n",
    "        self.add_weight_with_history(self.domain_weights, 0)\n",
    "    \n",
    "    def add_weight_with_history(self, domain_weights, step):\n",
    "        self.domain_weight_history.append(WeightHistory(step=step, weight=domain_weights.cpu()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e9b0e7-8ce0-442c-a35f-f19208937c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = build_tokenizer(\n",
    "        name = \"sp\",\n",
    "        path = \"/home/lama722b/nanite_lm/tokenizers/gemma/tokenizer.model\"\n",
    ")\n",
    "\n",
    "\n",
    "def compute_initial_weights(data_args):\n",
    "    num_samples_per_domain = [len(d) for d in datasets]\n",
    "    total_samples = sum(num_samples_per_domain)\n",
    "    weights = torch.tensor([num_sample / total_samples for num_sample in num_samples_per_domain])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4215e7e8-9a63-4063-95a2-4f7685054b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['de_shuffled', 'en_shuffled'] [1, 1] [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "sources = data_args.sources\n",
    "n_sources = len(sources)\n",
    "possible_sources = list(sources.keys())\n",
    "weights = list(sources.values())\n",
    "norm_weights = np.array(weights) / np.array(weights).sum()\n",
    "print(n_sources, possible_sources, weights, norm_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630bca8d-5fb4-45e9-a09f-453b59f5b7f5",
   "metadata": {},
   "source": [
    "### Trial by Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34635c2a-7e7b-463a-a1f6-d46778c34530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f76c995-2243-4acd-bedc-1fecb2bfa427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoReMiTransformer(BaseTransformer):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.weight_tying = args.weight_tying\n",
    "        self.sliding_window = args.sliding_window\n",
    "\n",
    "        assert args.vocab_size > 0\n",
    "\n",
    "        self.tok_embeddings = torch.nn.Embedding(args.vocab_size, args.dim)\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "        if args.weight_tying:\n",
    "            self.output = TiedLinear(self.tok_embeddings)\n",
    "        else:\n",
    "            self.output = nn.Linear(\n",
    "                args.dim,\n",
    "                args.vocab_size,\n",
    "                bias=False,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        token_values,\n",
    "        target = None,\n",
    "        tok_idx = None,\n",
    "        mask = None,\n",
    "        attn_impl = \"sdpa\",\n",
    "    ):\n",
    "        bsz, seqlen = token_values.shape\n",
    "\n",
    "        h = self.tok_embeddings(token_values)\n",
    "\n",
    "        mask = (\n",
    "            mask\n",
    "            if mask is not None\n",
    "            else create_causal_mask(seqlen, attn_impl, self.sliding_window)\n",
    "        )\n",
    "\n",
    "        h = super().forward(h, tok_idx=tok_idx, mask=mask, attn_impl=attn_impl)\n",
    "        logits = self.output(self.norm(h))\n",
    "        b()\n",
    "        if target is not None:\n",
    "            loss = per_token_cross_entropy(logits, target)\n",
    "            return loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7782e2b6-1cc1-47b6-848b-2b8db7570679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "tok_args = TokenizerArgs(\n",
    "    name = \"sp\",\n",
    "    path = \"/home/lama722b/nanite_lm/tokenizers/gemma/tokenizer.model\"\n",
    ")\n",
    "\n",
    "tokenizer = build_tokenizer(name=tok_args.name, path=tok_args.path)\n",
    "\n",
    "data_args = DataArgs(\n",
    "    root_dir = \"/home/lama722b/nanite_lm/data/fineweb\",\n",
    "    sources = {\n",
    "        \"de_shuffled\": 1,\n",
    "        \"en_shuffled\": 1\n",
    "    },\n",
    "    batch_size = 1,\n",
    "    seq_len=512,\n",
    "    load_async = False,\n",
    "    prefetch_size = 2,\n",
    "    tokenizer = tok_args\n",
    ")\n",
    "\n",
    "optim_args = OptimArgs()\n",
    "\n",
    "model_args = LMTransformerArgs(\n",
    "    vocab_size = tokenizer.n_words,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dim = 128,\n",
    "    max_seqlen = data_args.seq_len\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class TrainArgs:\n",
    "    data: DataArgs\n",
    "    model: LMTransformerArgs\n",
    "    optim: OptimArgs\n",
    "    steps: int\n",
    "\n",
    "args = TrainArgs(\n",
    "    data = data_args,\n",
    "    model = model_args,\n",
    "    optim = optim_args,\n",
    "    steps = NUM_TRAIN_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70de3054-4f54-4dfc-baa9-d6abec730f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainArgs(data=DataArgs(root_dir='/home/lama722b/nanite_lm/data/fineweb', sources={'de_shuffled': 1, 'en_shuffled': 1}, batch_size=1, seq_len=512, n_views=2, seed=42, add_bos=True, add_eos=True, load_async=False, prefetch_size=2, tokenizer=TokenizerArgs(name='sp', path='/home/lama722b/nanite_lm/tokenizers/gemma/tokenizer.model')), model=LMTransformerArgs(dim=128, n_layers=2, head_dim=None, n_heads=4, n_kv_heads=None, ffn_dim_multiplier=None, multiple_of=256, norm_eps=1e-05, rope_theta=10000.0, init_base_std=None, init_std_factor='disabled', max_seqlen=512, seed=42, vocab_size=262144, weight_tying=False, sliding_window=None), optim=OptimArgs(lr=0.0003, weight_decay=0.1, epsilon=1e-08, beta1=0.9, beta2=0.95, clip=1.0, scheduler='cosine', warmup=2000, lr_min_ratio=0.1, cycle_length=1.0, cosine_theta=1.0, annealing_step=1000, decay_fraction=0.1, exp_factor=0.5), steps=500)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82bd95-a5ec-4d13-ac93-a2d01ca60777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 iterations\n",
      "--Return--\n",
      "None\n",
      "> \u001b[32m/tmp/ipykernel_835939/3523260694.py\u001b[39m(\u001b[92m6\u001b[39m)\u001b[36mb\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pdb \u001b[38;5;28;01mimport\u001b[39;00m set_trace\n",
      "\u001b[32m----> 6\u001b[39m     set_trace()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/tmp/ipykernel_835939/3187652246.py\u001b[39m(\u001b[92m43\u001b[39m)\u001b[36mforward\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     42\u001b[39m         b()\n",
      "\u001b[32m---> 43\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m     44\u001b[39m             loss = per_token_cross_entropy(logits, target)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  target is None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l1 = cross_entropy(logits, target)\n",
      "ipdb>  l2 = per_token_cross_entropy(logits, target)\n",
      "ipdb>  l1.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.6476, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l2.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.4736, 12.9166, 12.3519, 12.5545, 12.1786, 12.3585, 12.6308, 12.1326,\n",
      "        13.0769, 12.6334, 12.7345, 12.1455, 12.5079, 12.4826, 13.2066, 13.3506,\n",
      "        12.6688, 12.0890, 11.9188, 12.3264, 12.9263, 13.2898, 12.4452, 12.8935,\n",
      "        12.2390, 12.4342, 13.4504, 12.6023, 11.3511, 13.3773, 13.7091, 12.4095,\n",
      "        13.2928, 12.7126, 11.4589, 13.6477, 12.5901, 12.9323, 12.1094, 11.8345,\n",
      "        12.0005, 12.8073, 12.3614, 12.3090, 12.6927, 12.6569, 12.3294, 12.9176,\n",
      "        13.7227, 12.9877, 12.7442, 12.1720, 12.5801, 12.8247, 12.2506, 12.5086,\n",
      "        12.8647, 12.6438, 12.6678, 13.0545, 13.0623, 12.2964, 13.5722, 12.4683,\n",
      "        13.3576, 12.5081, 13.1599, 12.3724, 13.3186, 12.3439, 12.8573, 12.8510,\n",
      "        11.7582, 11.9754, 12.5636, 12.2200, 12.5077, 12.5504, 13.0285, 12.6178,\n",
      "        13.2060, 13.4127, 12.9866, 12.8926, 13.1230, 12.7973, 13.2263, 12.8165,\n",
      "        12.8925, 12.3842, 12.6197, 11.7091, 14.1016, 11.7366, 13.2627, 13.0226,\n",
      "        12.6179, 13.2870, 12.1634, 12.0631, 11.9703, 12.4325, 12.3354, 12.2944,\n",
      "        12.8262, 13.1517, 12.6265, 12.6820, 11.8519, 12.4745, 12.6824, 12.7216,\n",
      "        13.1638, 12.9402, 12.9759, 12.6527, 12.4061, 13.4474, 12.5544, 12.4802,\n",
      "        12.4732, 12.1362, 12.7257, 12.9317, 12.8378, 12.7945, 12.0170, 12.6722,\n",
      "        12.8789, 12.4805, 12.5147, 12.2346, 12.5956, 12.8283, 12.8270, 13.1674,\n",
      "        12.7293, 12.6867, 13.0616, 12.3686, 13.1928, 11.4723, 12.0937, 12.0710,\n",
      "        12.3538, 12.2278, 12.7894, 12.2971, 13.2943, 13.3732, 12.9988, 12.5396,\n",
      "        11.9478, 13.1983, 13.8504, 12.5779, 13.5195, 12.8344, 13.2501, 12.0040,\n",
      "        13.7827, 11.9342, 12.1496, 11.9123, 12.2216, 12.5620, 12.6672, 13.1746,\n",
      "        13.2012, 13.2181, 12.4741, 12.1473, 12.6381, 12.3857, 13.9315, 12.6733,\n",
      "        11.6708, 12.5786, 12.4547, 12.5939, 12.7804, 13.1823, 12.8324, 13.0808,\n",
      "        11.9849, 11.7422, 12.5772, 13.4190, 13.2653, 13.0566, 13.0834, 11.3466,\n",
      "        12.7517, 12.2357, 12.5418, 11.4610, 12.2285, 13.1364, 11.4995, 13.4514,\n",
      "        12.5204, 13.3839, 12.9446, 12.9941, 12.3056, 12.7053, 13.5969, 13.5609,\n",
      "        13.1372, 11.7423, 12.3299, 13.5297, 12.6674, 12.0085, 12.3083, 13.3713,\n",
      "        13.6119, 13.2934, 12.6009, 12.5557, 12.4528, 13.1846, 11.4618, 12.2907,\n",
      "        13.0655, 12.9996, 12.1508, 13.4306, 13.0060, 12.6323, 12.8120, 12.8967,\n",
      "        12.4506, 12.8074, 13.0532, 12.8925, 13.1853, 11.5175, 12.2706, 12.2831,\n",
      "        12.8726, 13.1866, 12.7750, 13.3083, 12.3997, 12.4831, 12.1730, 12.3749,\n",
      "        12.7575, 12.5788, 12.3731, 12.7514, 12.9411, 12.9654, 12.3583, 11.7674,\n",
      "        12.9720, 11.9756, 12.0836, 12.8226, 11.7672, 12.4157, 13.4205, 12.2512,\n",
      "        11.9181, 12.3156, 13.3204, 12.4946, 12.5303, 12.5378, 12.5328, 12.7148,\n",
      "        12.1417, 12.9335, 12.3678, 12.4775, 13.0823, 12.7832, 13.0968, 13.0790,\n",
      "        13.0236, 12.5472, 12.5956, 12.4231, 12.8104, 12.4483, 12.4666, 13.0230,\n",
      "        12.4176, 12.6693, 12.7671, 12.8349, 12.9627, 12.5712, 12.9481, 13.2929,\n",
      "        12.7800, 12.8944, 12.1188, 12.4627, 12.4308, 12.1774, 11.8524, 12.1604,\n",
      "        12.7108, 12.4673, 13.5010, 13.2290, 12.8796, 11.4706, 12.4665, 12.3542,\n",
      "        12.4122, 13.0164, 12.7840, 12.0966, 12.8252, 12.6511, 12.1352, 12.7427,\n",
      "        11.9879, 12.8433, 13.6749, 12.3368, 11.9078, 12.8357, 12.2486, 13.3187,\n",
      "        12.4417, 11.7625, 12.3865, 12.3886, 12.9100, 13.7126, 13.1316, 12.0194,\n",
      "        13.5244, 12.3052, 13.2366, 12.2978, 12.5476, 12.7842, 13.0687, 12.5051,\n",
      "        12.8262, 12.6399, 13.3407, 12.5595, 12.8482, 12.6527, 13.3967, 13.2527,\n",
      "        12.2022, 12.8269, 12.3252, 13.0365, 12.5580, 12.9242, 12.3806, 13.5281,\n",
      "        12.8206, 12.6751, 12.3985, 12.9851, 14.4369, 12.1671, 12.2486, 13.1872,\n",
      "        12.9477, 11.9885, 12.9841, 13.1732, 12.7036, 12.9578, 11.6009, 12.5450,\n",
      "        12.6454, 12.0118, 11.7974, 12.6787, 12.0066, 12.3009, 12.7679, 12.7179,\n",
      "        12.4857, 12.1776, 12.2470, 13.1866, 12.9360, 11.9911, 12.9844, 13.1754,\n",
      "        12.7081, 12.0344, 12.4522, 12.3302, 12.9309, 12.2270, 12.6490, 12.9732,\n",
      "        12.4209, 12.3913, 12.2612, 12.4440, 12.4002, 12.4900, 13.7060, 12.1997,\n",
      "        12.9912, 12.5247, 12.2387, 13.0578, 12.6622, 12.7935, 12.1417, 12.0678,\n",
      "        13.0092, 12.1882, 11.7772, 12.3467, 13.1910, 12.9362, 12.0545, 13.3011,\n",
      "        11.0124, 13.4599, 11.8609, 12.1013, 12.4237, 12.6113, 13.2136, 12.4950,\n",
      "        13.0251, 13.9604, 13.2939, 13.0603, 12.7070, 12.4942, 12.9605, 12.0777,\n",
      "        12.7742, 12.9077, 13.0216, 12.9453, 12.5364, 13.4187, 12.0444, 13.3321,\n",
      "        13.6678, 11.8460, 11.9976, 12.6607, 12.1967, 11.8415, 12.9407, 13.1016,\n",
      "        12.8256, 12.4263, 12.3577, 11.5449, 13.4465, 11.6632, 12.5908, 12.7522,\n",
      "        11.9288, 12.3701, 13.0453, 12.5798, 12.3637, 13.2190, 13.1377, 12.5295,\n",
      "        12.5974, 11.8680, 12.6597, 13.2961, 11.6972, 12.3364, 12.9695, 12.6762,\n",
      "        12.0102, 12.2933, 12.8768, 11.8422, 12.8115, 12.8099, 11.3610, 12.8244,\n",
      "        12.3014, 12.7576, 12.6774, 13.0819, 11.7700, 12.4681, 11.9987, 12.8514,\n",
      "        12.6070, 13.2477, 12.6056, 12.7986, 11.8621, 11.9074, 13.2084, 13.1544,\n",
      "        13.6782, 12.9666, 11.2594, 13.1757, 12.7309, 13.2742, 12.4244, 13.4499],\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with ExitStack() as context_stack:\n",
    "    data_loader_state = init_dataloader_state_from_args(\n",
    "            args.data, rank=0, world_size=1 # Using dummy rank/degree for non-distributed\n",
    "        )\n",
    "    data_loader = context_stack.enter_context(\n",
    "                build_dataloader_from_args(\n",
    "                    args.data,\n",
    "                    state=data_loader_state,\n",
    "                )\n",
    "            )\n",
    "    model = DoReMiTransformer(args.model)\n",
    "    optimizer, scheduler = build_optimizer(model, args.optim, args.steps)\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Training the model for {args.steps} iterations\")\n",
    "    for n in range(args.steps):\n",
    "        batch, _ = next(data_loader)\n",
    "        batch = torch.tensor(batch)\n",
    "        input_ids = batch[:, :, 0]\n",
    "        labels = batch[:, :, 1]\n",
    "        loss = model(input_ids, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f\"Train step {n}; Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05cfea7-d21d-4a86-9e81-68c2e8089a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[32m/tmp/ipykernel_834270/2138924361.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mb\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pdb \u001b[38;5;28;01mimport\u001b[39;00m set_trace\n",
      "\u001b[32m      6\u001b[39m \n",
      "\u001b[32m----> 7\u001b[39m     set_trace()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 500 iterations\n",
      "Train step 1; Loss 12.68013858795166\n",
      "Train step 499; Loss 10.261428833007812\n"
     ]
    }
   ],
   "source": [
    "reference_model = train_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a709db56-a0cd-49f1-80eb-e26e0d7e7655",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LMTransformer' object has no attribute 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = LMTransformer(args.model)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweights\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/horse/ws/lama722b-nanite-lm/nanite-lm/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'LMTransformer' object has no attribute 'weights'"
     ]
    }
   ],
   "source": [
    "model = LMTransformer(args.model)\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc338ead-0523-4455-88cf-e894c2027176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
