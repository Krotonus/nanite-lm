{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa6632da-8518-42b9-b630-e5c73a1402ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/data/horse/ws/lama722b-nanite-lm/nanite-lm\")\n",
    "from pprint import pprint as print\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from xformers.ops import fmha, AttentionBias\n",
    "from torch.nn.attention.flex_attention import (\n",
    "    BlockMask,\n",
    "    flex_attention,\n",
    "    _mask_mod_signature,\n",
    ")\n",
    "from enum import Enum\n",
    "from typing import Optional, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee007476-b7d5-42fe-be47-2e8d0bcd86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebase.transformer import (\n",
    "    BaseTransformerArgs,\n",
    "    BaseTransformer,\n",
    "    TransformerBlock,\n",
    "    Attention,\n",
    "    FeedForward,\n",
    "    flex_attention_comp,\n",
    "    RMSNorm,\n",
    "    cross_entropy,\n",
    "    apply_rotary_emb,\n",
    "    reshape_for_broadcast,\n",
    "    repeat_kv,\n",
    "    InitStdFactor,\n",
    ")\n",
    "from codebase.optim import (\n",
    "    OptimArgs,\n",
    "    build_lr_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1022ed9a-5d82-496f-9db7-9d970e81deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seqlen, attn_impl, sliding_window):\n",
    "    if sliding_window is not None and attn_impl == \"fmha\":\n",
    "        return fmha.attn_bias.LocalAttentionFromBottomRightMask(\n",
    "            window_left=sliding_window - 1, window_right=0\n",
    "        )\n",
    "    elif attn_impl == \"fmha\":\n",
    "        return fmha.attn_bias.LowerTriangularMask()\n",
    "    elif attn_impl == \"sdpa\":\n",
    "        return \"causal\"\n",
    "    elif attn_impl == \"flex_attention\":\n",
    "        return create_block_mask(causal_mask, None, None, seqlen, seqlen)\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"Attention {attn_impl} with {sliding_window} sliding window not implemented\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46109bc-ab42-4356-b889-b7ffd473f186",
   "metadata": {},
   "source": [
    "## Mup Enabled Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b75b519-2c92-4a40-9341-9297dda5407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MupAttention(Attention):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        head_dim: int,\n",
    "        n_heads: int,\n",
    "        n_kv_heads: int,\n",
    "        rope_theta: float,\n",
    "        scaling_factor: float,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim,\n",
    "            head_dim,\n",
    "            n_heads,\n",
    "            n_kv_heads,\n",
    "            rope_theta,\n",
    "        )\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        freq_cis: torch.Tensor,\n",
    "        tok_idx: Optional[torch.Tensor] = None,\n",
    "        mask: Optional[Union[BlockMask, AttentionBias, str]] = None,\n",
    "        attn_impl: str = \"sdpa\",\n",
    "    ):\n",
    "        # B S D\n",
    "        bsz, seq_len, dim = x.shape\n",
    "        xq = self.wq(x.view_as(x))\n",
    "        xk = self.wk(x.view_as(x))\n",
    "        xv = self.wv(x.view_as(x))\n",
    "\n",
    "        output_shape = xq.shape\n",
    "        # B S D -> B S H D\n",
    "        xq = xq.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, 1, freq_cis[0:seq_len])\n",
    "\n",
    "        # This condition helps us be easily compatible\n",
    "        # with inference by adding a pluggable KVCache\n",
    "        if hasattr(self, \"kv_cache\"):\n",
    "            xk, xv = self.kv_cache.update(xk, xv, tok_idx)\n",
    "\n",
    "        xk = repeat_kv(xk, self.heads_per_group, dim=2)\n",
    "        xv = repeat_kv(xv, self.heads_per_group, dim=2)\n",
    "\n",
    "        attention_scaling_factor = 1.0 / self.n_heads\n",
    "\n",
    "        if attn_impl == \"flex_attention\":\n",
    "            assert mask is None or isinstance(mask, BlockMask)\n",
    "            xq, xk, xv = map(lambda e: e.transpose(1, 2), (xq, xk, xv))\n",
    "            output = flex_attention_comp(xq, xk, xv, block_mask=mask, scale=attention_scaling_factor)\n",
    "            output = output.transpose(1, 2).contiguous()  # B H S D -> B S H D\n",
    "\n",
    "        elif attn_impl == \"fmha\":\n",
    "            assert mask is None or isinstance(mask, AttentionBias)\n",
    "            output = fmha.memory_efficient_attention(xq, xk, xv, attn_bias=mask, scale=attention_scaling_factor)\n",
    "            # This uses B S H D instead of B H S D of pytorch\n",
    "\n",
    "        elif attn_impl == \"sdpa\":\n",
    "            xq, xk, xv = map(lambda e: e.transpose(1, 2), (xq, xk, xv))\n",
    "            assert mask is None or isinstance(mask, (str, torch.Tensor))\n",
    "            is_causal = (mask == \"causal\") if isinstance(mask, str) else False\n",
    "            mask = mask if isinstance(mask, torch.Tensor) else None\n",
    "            output = F.scaled_dot_product_attention(\n",
    "                xq,\n",
    "                xk,\n",
    "                xv,\n",
    "                is_causal=is_causal,\n",
    "                attn_mask=mask,\n",
    "                scale=attention_scaling_factor\n",
    "            )\n",
    "            output = output.transpose(1, 2).contiguous()  # B H S D -> B S H D\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Attention implementation {attn_impl} not supported\"\n",
    "            )\n",
    "\n",
    "        output = self.wo(output.reshape(output_shape))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset_parameters(self, init_std=None, out_proj_factor=1.0):\n",
    "        init_std = init_std or (self.dim ** (-0.5))\n",
    "\n",
    "        for w in [self.wq, self.wk, self.wv]:\n",
    "            nn.init.trunc_normal_(\n",
    "                w.weight,\n",
    "                mean=0.0,\n",
    "                std=init_std / math.sqrt(self.scaling_factor),\n",
    "                a=-3 * init_std,\n",
    "                b=3 * init_std,\n",
    "            )\n",
    "\n",
    "        nn.init.trunc_normal_(\n",
    "            self.wo.weight,\n",
    "            mean=0.0,\n",
    "            std=init_std / out_proj_factor,\n",
    "            a=-3 * init_std,\n",
    "            b=3 * init_std,\n",
    "        )\n",
    "\n",
    "class MupFeedForward(FeedForward):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        hidden_dim,\n",
    "        multiple_of,\n",
    "        ffn_dim_multiplier,\n",
    "        scaling_factor: float = 1.0,\n",
    "        mp_size: int = 1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim=dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            multiple_of=multiple_of,\n",
    "            ffn_dim_multiplier=ffn_dim_multiplier,\n",
    "            mp_size = mp_size,\n",
    "        )\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def reset_parameters(\n",
    "        self,\n",
    "        init_std=None,\n",
    "        factor=1.0,\n",
    "    ):\n",
    "        in_init_std = init_std or (self.dim ** (-0.5))\n",
    "        out_init_std = init_std or (self.hidden_dim ** (-0.5))\n",
    "        in_init_std = in_init_std / math.sqrt(self.scaling_factor)\n",
    "        out_init_std = out_init_std / factor\n",
    "        for w in [self.w1, self.w3]:\n",
    "            nn.init.trunc_normal_(\n",
    "                w.weight,\n",
    "                mean=0.0,\n",
    "                std=in_init_std,\n",
    "                a=-3 * in_init_std,\n",
    "                b=3 * in_init_std,\n",
    "            )\n",
    "        nn.init.trunc_normal_(\n",
    "            self.w2.weight,\n",
    "            mean=0.0,\n",
    "            std=out_init_std,\n",
    "            a=-3 * out_init_std,\n",
    "            b=3 * out_init_std,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6522b830-a6f5-441d-b21f-0121a9f65188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MupTransformerBlock(TransformerBlock):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.scaling_factor = args.scaling_factor\n",
    "        self.attention = MupAttention(\n",
    "            dim = args.dim,\n",
    "            head_dim = self.head_dim,\n",
    "            n_heads = self.n_heads,\n",
    "            n_kv_heads = self.n_kv_heads,\n",
    "            rope_theta = args.rope_theta,\n",
    "            scaling_factor = self.scaling_factor\n",
    "        )\n",
    "        self.feed_forward = MupFeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4*args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "            scaling_factor=args.scaling_factor\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c74ada17-4207-481c-b76e-76d52b4dabf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MupTransformerArgs(BaseTransformerArgs):\n",
    "    seed = 42\n",
    "    vocab_size = -1\n",
    "    weight_tying = False\n",
    "    sliding_window = None\n",
    "    input_alpha = 1.0\n",
    "    output_alpha = 1.0\n",
    "    scaling_factor: float = None\n",
    "\n",
    "class MupOptimArgs(OptimArgs):\n",
    "    scaling_factor: float = None\n",
    "\n",
    "\n",
    "class MupTransformer(BaseTransformer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "    ):\n",
    "        super().__init__(args)\n",
    "        self.input_alpha = args.input_alpha\n",
    "        self.output_alpha = args.output_alpha\n",
    "        self.weight_tying = args.weight_tying\n",
    "        self.sliding_window = args.sliding_window\n",
    "        self.scaling_factor = args.scaling_factor\n",
    "        \n",
    "\n",
    "        assert args.vocab_size > 0\n",
    "        assert args.scaling_factor >= 1, \"You need to set this!!!\"\n",
    "\n",
    "        self.tok_embeddings = torch.nn.Embedding(args.vocab_size, args.dim)\n",
    "        # This is Post-Layer Norm layer or Post-Norm\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "        if args.weight_tying:\n",
    "            self.output = TiedLinear(self.tok_embeddings)\n",
    "        else:\n",
    "            self.output = nn.Linear(\n",
    "                args.dim,\n",
    "                args.vocab_size,\n",
    "                bias=False\n",
    "            )\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(args.n_layers):\n",
    "            self.layers.append(MupTransformerBlock(args))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        token_values: torch.Tensor,\n",
    "        target = None,\n",
    "        tok_idx = None,\n",
    "        mask = None,\n",
    "        attn_impl = \"sdpa\",\n",
    "    ):\n",
    "        bsz, seqlen = token_values.shape\n",
    "        mask = (\n",
    "            mask\n",
    "            if mask is not None\n",
    "            else create_causal_mask(seqlen, attn_impl, self.sliding_window)\n",
    "        )\n",
    "        # (krotonus) NOTE: Embedding FWD MUP\n",
    "        h = self.input_alpha * self.tok_embeddings(token_values)\n",
    "\n",
    "        freq_cis = self.rope_embeddings(seqlen=self.max_seqlen, tok_idx=tok_idx)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h, freq_cis, tok_idx=tok_idx, mask=mask, attn_impl=attn_impl)\n",
    "\n",
    "        # (krotonus) NOTE: Output Logit FWD. MUP\n",
    "        logits = (self.output(self.norm(h)) * self.output_alpha) / self.scaling_factor\n",
    "        if target is not None:\n",
    "            return cross_entropy(logits, target)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.reset_parameters()\n",
    "        out_proj_factor =  math.sqrt(2 * args.n_layers * args.scaling_factor)\n",
    "        for depth, layer in enumerate(self.layers):\n",
    "            layer.init_weights(self.init_base_std, out_proj_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30d4be92-8aca-4d85-b21f-a491feb3b2b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MupTransformerArgs(dim=128,\n",
      "                   n_layers=2,\n",
      "                   head_dim=None,\n",
      "                   n_heads=4,\n",
      "                   n_kv_heads=None,\n",
      "                   ffn_dim_multiplier=None,\n",
      "                   multiple_of=256,\n",
      "                   norm_eps=1e-05,\n",
      "                   rope_theta=10000.0,\n",
      "                   init_base_std=None,\n",
      "                   init_std_factor='disabled',\n",
      "                   max_seqlen=128)\n",
      "'----------------------------------------------------------------------------------------------------'\n",
      "MupTransformer(\n",
      "  (rope_embeddings): RotaryEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x MupTransformerBlock(\n",
      "      (attention): MupAttention(\n",
      "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (wk): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (feed_forward): MupFeedForward(\n",
      "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (tok_embeddings): Embedding(256, 128)\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=128, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randint(low=0, high=256, size=(1, 10))\n",
    "\n",
    "args = MupTransformerArgs()\n",
    "args.n_heads = 4\n",
    "args.n_layers = 2\n",
    "args.dim = 128\n",
    "args.max_seqlen = 128\n",
    "args.vocab_size = 256\n",
    "args.scaling_factor = 1.0\n",
    "print(args)\n",
    "print(\"-\"*100)\n",
    "optim_args = MupOptimArgs()\n",
    "optim_args.scaling_factor = 1.0\n",
    "model = MupTransformer(args)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaa3ef96-002a-4641-b070-7502e3128641",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1a161fc-106a-4812-942c-ee58faa64b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10]), torch.Size([1, 10]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.size(), inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19092de3-5c15-41fe-bfb3-fccda235a57b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1746e-01,  3.7759e-02, -4.3266e-04,  ..., -3.6871e-01,\n",
       "           8.3709e-01, -1.3619e-01],\n",
       "         [ 4.0747e-01,  2.7429e-01, -7.2218e-01,  ...,  9.1531e-01,\n",
       "           6.4633e-01, -1.0130e+00],\n",
       "         [ 4.1656e-01,  3.1421e-01, -4.0535e-01,  ..., -1.5261e-01,\n",
       "           1.6991e-01, -5.8452e-01],\n",
       "         ...,\n",
       "         [-2.8328e-01, -2.4091e-01,  5.4915e-01,  ...,  1.3069e+00,\n",
       "           4.3992e-01, -5.0319e-01],\n",
       "         [-2.3307e-01,  9.5362e-01, -1.7055e-01,  ...,  8.9563e-02,\n",
       "          -7.0343e-01,  3.6659e-01],\n",
       "         [ 3.4711e-01,  4.3249e-01, -9.6958e-01,  ...,  8.5858e-01,\n",
       "           1.9173e+00, -2.0373e-01]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dba7153-8418-43be-ad27-ada73114b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'layers.0.attention.wq.weight : torch.Size([128, 128])'\n",
      "'layers.0.attention.wk.weight : torch.Size([128, 128])'\n",
      "'layers.0.attention.wv.weight : torch.Size([128, 128])'\n",
      "'layers.0.attention.wo.weight : torch.Size([128, 128])'\n",
      "'layers.0.feed_forward.w1.weight : torch.Size([512, 128])'\n",
      "'layers.0.feed_forward.w3.weight : torch.Size([512, 128])'\n",
      "'layers.0.feed_forward.w2.weight : torch.Size([128, 512])'\n",
      "'layers.0.attention_norm.weight : torch.Size([128])'\n",
      "'layers.0.ffn_norm.weight : torch.Size([128])'\n",
      "'layers.1.attention.wq.weight : torch.Size([128, 128])'\n",
      "'layers.1.attention.wk.weight : torch.Size([128, 128])'\n",
      "'layers.1.attention.wv.weight : torch.Size([128, 128])'\n",
      "'layers.1.attention.wo.weight : torch.Size([128, 128])'\n",
      "'layers.1.feed_forward.w1.weight : torch.Size([512, 128])'\n",
      "'layers.1.feed_forward.w3.weight : torch.Size([512, 128])'\n",
      "'layers.1.feed_forward.w2.weight : torch.Size([128, 512])'\n",
      "'layers.1.attention_norm.weight : torch.Size([128])'\n",
      "'layers.1.ffn_norm.weight : torch.Size([128])'\n",
      "'tok_embeddings.weight : torch.Size([256, 128])'\n",
      "'norm.weight : torch.Size([128])'\n",
      "'output.weight : torch.Size([256, 128])'\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} : {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2f987-38db-4964-ba14-8dca997d7252",
   "metadata": {},
   "source": [
    "### Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fa5caed-988b-47c0-82d8-1e3ec3416c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mup_optimizer(\n",
    "    model: nn.Module,\n",
    "    args: OptimArgs,\n",
    "    n_steps: int,\n",
    "):\n",
    "    mup_decay_params = []\n",
    "    decay_params = []\n",
    "    nodecay_params = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.dim() >= 2:\n",
    "            if (\n",
    "            n.endswith('wq.weight') or\n",
    "            n.endswith('wk.weight') or\n",
    "            n.endswith('wv.weight') or\n",
    "            n.endswith('wo.weight') or\n",
    "            n.endswith('w1.weight') or\n",
    "            n.endswith('w2.weight') or\n",
    "            n.endswith('w3.weight')\n",
    "            ):\n",
    "                print(f\"Added {n} to mup_decay_list\")\n",
    "                mup_decay_params.append(p)\n",
    "            else:\n",
    "                decay_params.append(p)\n",
    "        else:\n",
    "            nodecay_params.append(p)\n",
    "    optim_groups = [\n",
    "        {'params': mup_decay_params, 'weight_decay': args.weight_decay, 'lr_scale': (1/args.scaling_factor)},\n",
    "        {'params': decay_params, 'weight_decay': args.weight_decay, 'lr_scale': 1},\n",
    "        {'params': nodecay_params, 'weight_decay': 0.0, 'lr_scale': 1}\n",
    "    ]\n",
    "    num_mup_decay_params = sum(p.numel() for p in mup_decay_params)\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "    print(f\"num mup decayed parameter tensors: {len(mup_decay_params)}, with {num_mup_decay_params:,} parameters\")\n",
    "    print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "    print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optim_groups,\n",
    "        lr=args.lr,\n",
    "        betas=(args.beta1, args.beta2),\n",
    "        eps=args.epsilon,\n",
    "        fused=True,  # Faster optim.step but can throw errors\n",
    "    )\n",
    "\n",
    "    # scheduler\n",
    "    lr_fn = build_lr_fn(args, n_steps)\n",
    "    scheduler = lr_scheduler.LambdaLR(\n",
    "        optimizer, lr_fn\n",
    "    )  # lr_scheduler.LambdaLR(optimizer, lr_fn)\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d8ec524-75a6-494b-8ebf-0e635c0a5f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Added layers.0.attention.wq.weight to mup_decay_list'\n",
      "'Added layers.0.attention.wk.weight to mup_decay_list'\n",
      "'Added layers.0.attention.wv.weight to mup_decay_list'\n",
      "'Added layers.0.attention.wo.weight to mup_decay_list'\n",
      "'Added layers.0.feed_forward.w1.weight to mup_decay_list'\n",
      "'Added layers.0.feed_forward.w3.weight to mup_decay_list'\n",
      "'Added layers.0.feed_forward.w2.weight to mup_decay_list'\n",
      "'Added layers.1.attention.wq.weight to mup_decay_list'\n",
      "'Added layers.1.attention.wk.weight to mup_decay_list'\n",
      "'Added layers.1.attention.wv.weight to mup_decay_list'\n",
      "'Added layers.1.attention.wo.weight to mup_decay_list'\n",
      "'Added layers.1.feed_forward.w1.weight to mup_decay_list'\n",
      "'Added layers.1.feed_forward.w3.weight to mup_decay_list'\n",
      "'Added layers.1.feed_forward.w2.weight to mup_decay_list'\n",
      "'num mup decayed parameter tensors: 14, with 524,288 parameters'\n",
      "'num decayed parameter tensors: 2, with 65,536 parameters'\n",
      "'num non-decayed parameter tensors: 5, with 640 parameters'\n"
     ]
    }
   ],
   "source": [
    "optimizer, scheduler = build_mup_optimizer(model, optim_args, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3b785e7-6624-4f03-84ec-56d8eb69bf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''\n",
      "'rope_embeddings'\n",
      "'layers'\n",
      "'layers.0'\n",
      "'layers.0.attention'\n",
      "'layers.0.attention.wq'\n",
      "'layers.0.attention.wk'\n",
      "'layers.0.attention.wv'\n",
      "'layers.0.attention.wo'\n",
      "'layers.0.feed_forward'\n",
      "'layers.0.feed_forward.w1'\n",
      "'layers.0.feed_forward.w3'\n",
      "'layers.0.feed_forward.w2'\n",
      "'layers.0.attention_norm'\n",
      "'layers.0.ffn_norm'\n",
      "'layers.1'\n",
      "'layers.1.attention'\n",
      "'layers.1.attention.wq'\n",
      "'layers.1.attention.wk'\n",
      "'layers.1.attention.wv'\n",
      "'layers.1.attention.wo'\n",
      "'layers.1.feed_forward'\n",
      "'layers.1.feed_forward.w1'\n",
      "'layers.1.feed_forward.w3'\n",
      "'layers.1.feed_forward.w2'\n",
      "'layers.1.attention_norm'\n",
      "'layers.1.ffn_norm'\n",
      "'tok_embeddings'\n",
      "'norm'\n",
      "'output'\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7920f-638e-4629-99a7-38f90f05997c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
